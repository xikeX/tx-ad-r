# main.py
import os

from infer_class import Infer
if os.environ.get("DEBUG_MODE","") == 'True':
    print("in debug model")
    os.environ['TRAIN_LOG_PATH']='./log_path'
    os.environ['TRAIN_TF_EVENTS_PATH']='./log_path'
    os.environ['TRAIN_DATA_PATH']='./TencentGR_1k'
    os.environ['TRAIN_CKPT_PATH']='./checkpoint'
    os.environ['USER_CACHE_PATH']='./user_cache_file'
    os.environ['DEBUG_MODE'] = 'True'
    # è®¾ç½®å•GPU
os.environ['TEMP_PATH'] = './temp'
os.makedirs(os.environ['TEMP_PATH'], exist_ok = True)
os.environ['EVAL_RESULT_PATH'] = './eval_result'

"""
è®­ç»ƒä¸»è„šæœ¬ï¼šç”¨äºè®­ç»ƒåµŒå…¥æ¨¡å‹ï¼ˆEmbedding Modelï¼‰ï¼Œå¹¶åŠ è½½åˆ°ä¸‹æ¸¸æ¨¡å‹ä¸­è¿›è¡Œåç»­ä»»åŠ¡ã€‚
"""

import os
import argparse
import json
import time
from pathlib import Path
import random
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from torch.nn.utils import clip_grad_norm_

# è‡ªå®šä¹‰æ¨¡å—
from my_dataset import BaseDataset, TrainDataset, EmbeddingDataset, ValidDataset
from embedding_model import BaselineModel as EmbeddingModel
from model import BaselineModel as DownstreamModel
from transformers import get_cosine_schedule_with_warmup
import torch
import torch.nn as nn
from pathlib import Path
import json
import os
import time
from tqdm import tqdm
def set_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒå¯å¤ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    torch.set_num_threads(1)


def get_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="è®­ç»ƒåºåˆ—æ¨èæ¨¡å‹ï¼ˆå«ç‰¹å¾åµŒå…¥ï¼‰")

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--batch_size', type=int, default=32, help='è®­ç»ƒ/éªŒè¯æ‰¹å¤§å°')
    # è®­ç»ƒemmbeddingçš„batch_size
    parser.add_argument('--embedding_batch_size', type=int, default=5, help='è®­ç»ƒembeddingçš„æ‰¹å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-2, help='å­¦ä¹ ç‡')
    parser.add_argument('--maxlen', type=int, default=101, help='åºåˆ—æœ€å¤§é•¿åº¦')

    # æ¨¡å‹ç»“æ„å‚æ•°
    parser.add_argument('--hidden_units', type=int, default=64, help='éšè—å±‚ç»´åº¦')
    parser.add_argument('--num_blocks', type=int, default=1, help='Transformer å—æ•°')
    parser.add_argument('--num_epochs', type=int, default=25, help='è®­ç»ƒæ€»è½®æ•°')
    parser.add_argument('--num_heads', type=int, default=4, help='å¤šå¤´æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--dropout_rate', type=float, default=0.1, help='Dropout æ¯”ä¾‹')
    parser.add_argument('--l2_emb', type=float, default=0.0, help='åµŒå…¥å±‚L2æ­£åˆ™å¼ºåº¦')
    parser.add_argument('--device', type=str, default='cpu', help='è¿è¡Œè®¾å¤‡: cpu æˆ– cuda')
    parser.add_argument('--inference_only', action='store_true', help='ä»…æ¨ç†æ¨¡å¼')
    parser.add_argument('--state_dict_path', type=str, default=None, help='é¢„è®­ç»ƒæƒé‡è·¯å¾„')
    parser.add_argument('--norm_first', action='store_true', help='æ˜¯å¦åœ¨Transformerä¸­å…ˆå½’ä¸€åŒ–')
    parser.add_argument('--mm_emb_id', nargs='+', type=str, default=['81'],
                        choices=[str(s) for s in range(81, 87)],
                        help='å¤šæ¨¡æ€åµŒå…¥ç‰¹å¾IDåˆ—è¡¨')
    parser.add_argument("--warm_up_rate", default=0.1, type=float, help="warm up çš„æ¯”ä¾‹")
    
    # æ¨¡å‹æ¨¡å—/è®­ç»ƒæ¨¡å—å‚æ•°
    parser.add_argument("--use_embedding_model", action="store_true", help="æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡")
    # ä½¿ç”¨åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä½¿ç”¨å­¦ä¹ ç‡ç­–ç•¥
    parser.add_argument("--use_lr_scheduler_in_downstream", action="store_true", help="æ˜¯å¦ä½¿ç”¨åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä½¿ç”¨å­¦ä¹ ç‡ç­–ç•¥")
    return parser.parse_args()


def initialize_model_weights(model: nn.Module):
    """é€’å½’åˆå§‹åŒ–æ¨¡å‹æƒé‡"""

    def init_weights(m):
        if isinstance(m, nn.Linear):
            nn.init.xavier_normal_(m.weight.data)
            if m.bias is not None:
                nn.init.zeros_(m.bias.data)
        elif isinstance(m, nn.Embedding):
            # ä½¿ç”¨å°æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–åµŒå…¥å±‚
            nn.init.normal_(m.weight.data, mean=0.0, std=0.02)
        elif isinstance(m, (nn.LSTM, nn.GRU)):
            for name, param in m.named_parameters():
                if 'weight' in name:
                    nn.init.xavier_normal_(param.data)
                elif 'bias' in name:
                    nn.init.zeros_(param.data)

    model.apply(init_weights)

    # ç‰¹æ®Šå¤„ç†ï¼špadding index (0) çš„åµŒå…¥ç½®é›¶
    if hasattr(model, 'pos_emb') and model.pos_emb.weight.data is not None:
        model.pos_emb.weight.data[0, :] = 0
    if hasattr(model, 'item_emb') and model.item_emb.weight.data is not None:
        model.item_emb.weight.data[0, :] = 0
    if hasattr(model, 'user_emb') and model.user_emb.weight.data is not None:
        model.user_emb.weight.data[0, :] = 0
    if hasattr(model, 'sparse_emb'):
        for emb_layer in model.sparse_emb.values():
            emb_layer.weight.data[0, :] = 0


def train_embedding_model(embedding_model, train_loader, valid_loader, optimizer, scheduler, args, writer, log_file, global_step):
    """è®­ç»ƒåµŒå…¥æ¨¡å‹ä¸»å¾ªç¯"""
    print("å¼€å§‹è®­ç»ƒåµŒå…¥æ¨¡å‹...")
    bce_criterion = nn.BCEWithLogitsLoss(reduction='mean')
    best_val_loss = float('inf')
    # è®¾ç½®ç­–ç•¥warmup å’Œ cosine decayç­–ç•¥

    for epoch in range(1, args.num_epochs + 1):
        embedding_model.train()
        if args.inference_only:
            print("inference_only æ¨¡å¼å¼€å¯ï¼Œè·³è¿‡è®­ç»ƒã€‚")
            break

        t0 = time.time()
        total_loss_epoch = 0.0

        # å¯ç”¨æ¢¯åº¦ç¼©æ”¾ï¼ˆç”¨äºæ··åˆç²¾åº¦æˆ–é˜²æ­¢æº¢å‡ºï¼‰
        scaler = torch.cuda.amp.GradScaler() if args.device == 'cuda' else None
        local_step = 0
        with tqdm(train_loader,total=-1, desc=f"Epoch {epoch}", leave=False) as pbar:
            for step, batch in enumerate(pbar):
                try:
                    # è§£åŒ…æ•°æ®
                    user, input_item, pos_item, neg_item, \
                    user_feat, input_item_feat, pos_item_feat, neg_item_feat = batch

                    # ======== å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦å¯é€‰ï¼‰========
                    if scaler:
                        with torch.cuda.amp.autocast():
                            info_nce_loss = embedding_model(
                                user, input_item, pos_item, neg_item,
                                user_feat, input_item_feat, pos_item_feat, neg_item_feat
                            )
                            loss = info_nce_loss
                    else:
                        info_nce_loss = embedding_model(
                                user, input_item, pos_item, neg_item,
                                user_feat, input_item_feat, pos_item_feat, neg_item_feat
                            )
                        loss = info_nce_loss

                        # L2 æ­£åˆ™åŒ–ï¼šä»…ä½œç”¨äº item_embï¼Œé¿å…è¿‡å¼ºæƒ©ç½š
                        if args.l2_emb > 0:
                            l2_reg = 0.0
                            for param in embedding_model.item_emb.parameters():
                                l2_reg += torch.norm(param)
                            loss += args.l2_emb * l2_reg

                    # ======== åå‘ä¼ æ’­ ========
                    optimizer.zero_grad()
                    local_step +=1
                    if scaler is not None:
                        # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                        scaler.scale(loss).backward()
                        # æ¢¯åº¦è£å‰ªï¼ˆåœ¨ unscale ä¹‹åï¼‰
                        scaler.unscale_(optimizer)
                        clip_grad_norm_(embedding_model.parameters(), max_norm=1.0)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        # æ™®é€šç²¾åº¦
                        loss.backward()
                        clip_grad_norm_(embedding_model.parameters(), max_norm=1.0)
                        optimizer.step()

                    # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                    if scheduler is not None:
                        scheduler.step()

                    # ======== æ—¥å¿—è®°å½• ========
                    total_loss_epoch += loss.item()

                    # å½“å‰å­¦ä¹ ç‡
                    current_lr = scheduler.get_last_lr()[0] if scheduler is not None else args.lr

                    log_json = json.dumps({
                        'global_step': global_step,
                        'total_loss': round(loss.item(), 6),
                        'info_nce_loss': round(info_nce_loss.item(), 6),
                        'l2_reg_loss': round((loss - info_nce_loss).item(), 6) if args.l2_emb > 0 else 0.0,
                        'learning_rate': round(current_lr, 8),
                        'epoch': epoch,
                        'time': time.time()
                    })
                    log_file.write(log_json + '\n')
                    log_file.flush()

                    writer.add_scalar('Loss/Train', loss.item(), global_step)
                    writer.add_scalar('Loss/InfoNCE', info_nce_loss.item(), global_step)
                    if args.l2_emb > 0:
                        writer.add_scalar('Loss/L2_Reg', (loss - info_nce_loss).item(), global_step)
                    writer.add_scalar('Learning_rate', current_lr, global_step)

                    global_step += 1
                    pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{current_lr:.2e}'})
                    if global_step % 200 == 0:
                        for name, param in embedding_model.named_parameters():
                            writer.add_histogram(f'params/{name}', param, global_step)
                            if param.grad is not None:
                                writer.add_histogram(f'grads/{name}', param.grad, global_step)
                                print(f"grad/{name} : {param.grad.mean()}")
                except Exception as e:
                    print(f"\nâŒ Error at epoch {epoch}, step {step}: {str(e)}")
                    # å¯é€‰ï¼šä¿å­˜å½“å‰ batch æ•°æ®ç”¨äº debug
                    # torch.save(batch, "debug_batch.pt")
                    raise  #    


        avg_train_loss = total_loss_epoch / local_step
        print(f"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Time: {time.time() - t0:.2f}s")

        # éªŒè¯é˜¶æ®µ
        embedding_model.eval()
        val_loss = 0.0
        vel_local_step = 0
        with torch.no_grad():
            for batch in tqdm(valid_loader, total=-1, desc="Validation", leave=True):
                user, input_item, pos_item, neg_item, \
                user_feat, input_item_feat, pos_item_feat, neg_item_feat = batch

                info_nce_loss = embedding_model(
                    user, input_item, pos_item, neg_item,
                    user_feat, input_item_feat, pos_item_feat, neg_item_feat
                )
                val_loss += info_nce_loss.item()
                vel_local_step += 1

        val_loss /= vel_local_step
        writer.add_scalar('Loss/Valid', val_loss, global_step)
        print(f"Epoch {epoch} | Valid Loss: {val_loss:.4f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            save_dir = Path(os.environ.get('USER_CACHE_PATH')) / "best_embedding_model"
            save_dir.mkdir(parents=True, exist_ok=True)
            torch.save(embedding_model.state_dict(), save_dir / "model.pt")
            print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_dir / 'model.pt'}")

    return global_step


def run_embedding_training(
        args,
        train_emb_dataset,
        valid_emb_dataset,
        writer: SummaryWriter,
        log_file,
        final_save_dir,
        device=None
    ):
    """
    å°è£…åµŒå…¥æ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼šåˆå§‹åŒ–ã€åŠ è½½æƒé‡ã€è®­ç»ƒã€ä¿å­˜ã€‚

    Args:
        args: è®­ç»ƒå‚æ•°å‘½åç©ºé—´ï¼Œéœ€åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - batch_size
            - lr
            - state_dict_path (å¯é€‰)
            - USER_CACHE_PATH
            - å…¶ä»–æ¨¡å‹ç›¸å…³å‚æ•°
        train_emb_dataset: è®­ç»ƒæ•°æ®é›†ï¼Œéœ€æœ‰ collate_fn
        valid_emb_dataset: éªŒè¯æ•°æ®é›†ï¼Œéœ€æœ‰ collate_fn
        writer: TensorBoard SummaryWriter å®ä¾‹
        log_file: æ—¥å¿—æ–‡ä»¶å¯¹è±¡ï¼ˆå·²æ‰“å¼€ï¼‰
        device: è®­ç»ƒè®¾å¤‡ (å¦‚ 'cuda' æˆ– 'cpu')

    Returns:
        embedding_model: è®­ç»ƒå®Œæˆçš„æ¨¡å‹
        global_step: æ€»è®­ç»ƒæ­¥æ•°
    """
    device = args.device
    print("ğŸš€ å¼€å§‹æ„å»ºåµŒå…¥æ¨¡å‹è®­ç»ƒæµç¨‹...")

    # ==========================
    # 1. æ•°æ®åŠ è½½å™¨
    # ==========================
    train_loader = DataLoader(
        train_emb_dataset,
        batch_size=args.embedding_batch_size,
        num_workers=0,
        collate_fn=train_emb_dataset.collate_fn,
        shuffle=False  # ç¡®ä¿è®­ç»ƒé›†æ‰“ä¹±
    )

    valid_loader = DataLoader(
        valid_emb_dataset,
        batch_size=args.batch_size,
        num_workers=0,
        collate_fn=valid_emb_dataset.collate_fn,
        shuffle=False
    )

    print(f"ğŸ“Š è®­ç»ƒé›† batch æ•°: {len(train_loader.dataset.sample_index)}, éªŒè¯é›† batch æ•°: {len(valid_loader.dataset.sample_index)}")

    # ==========================
    # 2. æ¨¡å‹åˆå§‹åŒ–
    # ==========================
    embedding_model = EmbeddingModel(
        user_num=train_emb_dataset.base_dataset.usernum,
        item_num=train_emb_dataset.base_dataset.itemnum,
        feat_statistics=train_emb_dataset.base_dataset.feat_statistics,
        feat_types=train_emb_dataset.base_dataset.feature_types,
        args=args
    ).to(device)

    # è‡ªå®šä¹‰åˆå§‹åŒ–
    initialize_model_weights(embedding_model)
    print("âœ… æ¨¡å‹æƒé‡å·²åˆå§‹åŒ–")

    # ==========================
    # 3. åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¯é€‰ï¼‰
    # ==========================
    if args.state_dict_path:
        try:
            state_dict = torch.load(args.state_dict_path, map_location=device)
            embedding_model.load_state_dict(state_dict)
            print(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {args.state_dict_path}")
            log_file.write(f"INFO: Loaded pretrained weights from {args.state_dict_path}\n")
        except Exception as e:
            msg = f"âŒ åŠ è½½é¢„è®­ç»ƒæƒé‡å¤±è´¥: {args.state_dict_path}, é”™è¯¯: {str(e)}"
            print(msg)
            log_file.write(f"ERROR: {msg}\n")
            raise RuntimeError(msg)

    # ==========================
    # 4. ä¼˜åŒ–å™¨
    # ==========================
    optimizer = torch.optim.Adam(
        embedding_model.parameters(),
        lr=args.lr,
        betas=(0.9, 0.98),
        weight_decay=getattr(args, 'weight_decay', 0.0)  # å¯é€‰ weight decay
    )
    # scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=args.warm_up_rate * len(train_loader),
    #                                         num_training_steps=args.num_epochs * len(train_loader))
    scheduler = None
    # ==========================
    # 5. å¼€å§‹è®­ç»ƒ
    # ==========================
    global_step = 0
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    try:
        global_step = train_embedding_model(
            embedding_model=embedding_model,
            train_loader=train_loader,
            valid_loader=valid_loader,
            optimizer=optimizer,
            scheduler=scheduler,
            args=args,
            writer=writer,
            log_file=log_file,
            global_step=global_step
        )
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise

    # ==========================
    # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    # ==========================
    
    final_save_dir.mkdir(parents=True, exist_ok=True)

    save_path = final_save_dir / "model.pt"
    torch.save(embedding_model.state_dict(), save_path)
    print(f"âœ… æœ€ç»ˆåµŒå…¥æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
    log_file.write(f"INFO: Final model saved to {save_path}\n")

    return embedding_model, global_step



def train_downstream_model(model, train_loader, valid_loader, optimizer, args, log_file, writer,test_dataset = None, test_dataset_2=None):
    global_step = 0
    print("å¼€å§‹ä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒ...")

    # âœ… æ·»åŠ ï¼šå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¯é€‰ï¼šReduceLROnPlateauï¼‰
    scheduler = None
    if args.use_lr_scheduler_in_downstream:
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.num_epochs * len(train_loader),eta_min=1e-5)

    best_val_loss = float('inf')  # âœ… è®°å½•æœ€ä½³éªŒè¯æŸå¤±
    patience = args.early_stop_patience if hasattr(args, 'early_stop_patience') else 5
    no_improve_count = 0

    bce_criterion = nn.BCEWithLogitsLoss()  # å»ºè®®ä½¿ç”¨ BCEWithLogitsLossï¼ˆæ›´ç¨³å®šï¼‰

    for epoch in range(1, args.num_epochs + 1):
        model.train()
        t0 = time.time()
        total_loss_epoch = 0.0

        with tqdm(train_loader, desc=f"Epoch {epoch}", leave=False) as pbar:
            for step, batch in enumerate(pbar):
                try:
                    # è§£åŒ…æ•°æ®
                    seq, pos, neg, token_type, next_token_type, next_action_type, \
                    seq_feat, pos_feat, neg_feat = batch

                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    seq = seq.to(args.device)
                    pos = pos.to(args.device)
                    neg = neg.to(args.device)
                    token_type = token_type.to(args.device)
                    next_token_type = next_token_type.to(args.device)
                    next_action_type = next_action_type.to(args.device)
                    # seq_feat = seq_feat.to(args.device)
                    # pos_feat = pos_feat.to(args.device)
                    # neg_feat = neg_feat.to(args.device)

                    # å‰å‘ä¼ æ’­
                    pos_logits, neg_logits, sim_loss = model(
                        seq, pos, neg, token_type, next_token_type, next_action_type,
                        seq_feat, pos_feat, neg_feat
                    )

                    # åªå¯¹ next_token_type == 1 çš„ä½ç½®è®¡ç®—æŸå¤±
                    indices = (next_token_type == 1)  # [B, L]

                    # ä½¿ç”¨ BCEWithLogitsLossï¼ˆè‡ªå¸¦ sigmoidï¼Œæ•°å€¼æ›´ç¨³å®šï¼‰
                    loss_pos = bce_criterion(pos_logits[indices], torch.ones_like(pos_logits[indices]))
                    loss_neg = bce_criterion(neg_logits[indices], torch.zeros_like(neg_logits[indices]))
                    bce_loss = loss_pos + loss_neg

                    # ç›¸ä¼¼æ€§æŸå¤±
                    sim_loss = sim_loss.mean() * getattr(args, 'sim_loss_weight', 10.0)

                    total_loss = bce_loss 

                    # L2 æ­£åˆ™åŒ–ï¼ˆä»… item_embï¼‰
                    if args.l2_emb > 0:
                        l2_reg = 0.0
                        for param in model.item_emb.parameters():
                            l2_reg += torch.norm(param)
                        total_loss += args.l2_emb * l2_reg

                    # âœ… æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸º NaN æˆ– Inf
                    if torch.isnan(total_loss) or torch.isinf(total_loss):
                        print(f"âŒ è·³è¿‡ batchï¼ŒæŸå¤±å¼‚å¸¸ï¼ˆNaN/Infï¼‰ at epoch {epoch}, step {step}")
                        continue

                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    total_loss.backward()

                    # âœ… æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
                    # if epoch>30:
                    #     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    
                    # è¾“å‡ºæ‰€æœ‰å±‚çš„æ¢¯åº¦åˆ°writer
                    for name, param in model.named_parameters():
                        if param.requires_grad and param.grad is not None:
                            writer.add_scalar(name + '_grad', param.grad.norm(), global_step)
                    optimizer.step()
                    # scheduler.step()

                    # æ—¥å¿—è®°å½•
                    log_entry = {
                        'global_step': global_step,
                        'bce_loss': bce_loss.item(),
                        'sim_loss': sim_loss.item(),
                        'total_loss': total_loss.item(),
                        'epoch': epoch,
                        'time': time.time()
                    }
                    log_file.write(json.dumps(log_entry) + '\n')
                    log_file.flush()

                    # TensorBoard
                    writer.add_scalar('Loss/BCE', bce_loss.item(), global_step)
                    writer.add_scalar('Loss/Sim', sim_loss.item(), global_step)
                    writer.add_scalar('Loss/Total', total_loss.item(), global_step)
                    writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], global_step)
                    total_loss_epoch += total_loss.item()
                    global_step += 1

                    pbar.set_postfix({
                        'bce': f'{bce_loss.item():.4f}',
                        'sim': f'{sim_loss.item():.4f}'
                    })

                except Exception as e:
                    print(f"âŒ è®­ç»ƒå¼‚å¸¸ï¼ˆè·³è¿‡ batchï¼‰: {e}")
                    continue  # è·³è¿‡å½“å‰ batch

        avg_train_loss = total_loss_epoch / len(train_loader)
        print(f"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Time: {time.time() - t0:.2f}s")

        # ========== éªŒè¯é˜¶æ®µ ==========
        model.eval()
        val_loss_sum = 0.0
        val_batches = 0
        with torch.no_grad():
            for batch in tqdm(valid_loader, desc="Validation", leave=False):
                try:
                    seq, pos, neg, token_type, next_token_type, next_action_type, \
                    seq_feat, pos_feat, neg_feat = batch

                    seq = seq.to(args.device)
                    pos = pos.to(args.device)
                    neg = neg.to(args.device)
                    next_token_type = next_token_type.to(args.device)

                    pos_logits, neg_logits, sim_loss = model(
                        seq, pos, neg, token_type, next_token_type, next_action_type,
                        seq_feat, pos_feat, neg_feat
                    )

                    indices = (next_token_type == 1)
                    if not indices.any():
                        continue  # è·³è¿‡æ— ç›®æ ‡åŠ¨ä½œçš„ batch

                    loss_pos = bce_criterion(pos_logits[indices], torch.ones_like(pos_logits[indices]))
                    loss_neg = bce_criterion(neg_logits[indices], torch.zeros_like(neg_logits[indices]))
                    bce_loss = loss_pos + loss_neg

                    val_loss_sum += bce_loss.item()
                    val_batches += 1

                except Exception as e:
                    print(f"âŒ éªŒè¯å¼‚å¸¸ï¼ˆè·³è¿‡ batchï¼‰: {e}")
                    continue

        avg_val_loss = val_loss_sum / val_batches if val_batches > 0 else float('inf')
        writer.add_scalar('Loss/Valid', avg_val_loss, global_step)
        print(f"Epoch {epoch} | Valid BCE Loss: {avg_val_loss:.4f}")

        # âœ… å­¦ä¹ ç‡è°ƒåº¦
        if scheduler is not None:
            scheduler.step(avg_val_loss)

        # âœ… ä¿å­˜æœ€ä½³æ¨¡å‹
        ckpt_dir = Path(os.environ.get('TRAIN_CKPT_PATH')) / f"global_best_model"
        cache_dir = Path(os.environ.get('USER_CACHE_PATH')) / f"global_best_model"

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            no_improve_count = 0

            for save_dir in [ckpt_dir, cache_dir]:
                save_dir.mkdir(parents=True, exist_ok=True)
                torch.save(model.state_dict(), save_dir / "model.pt")
                print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_dir / 'model.pt'}")
        else:
            ckpt_dir = Path(os.environ.get('TRAIN_CKPT_PATH')) / f"global_step{global_step}_sim_model.valid_loss={avg_val_loss:.4f}"
            no_improve_count += 1
            for save_dir in [ckpt_dir]:
                save_dir.mkdir(parents=True, exist_ok=True)
                torch.save(model.state_dict(), save_dir / "model.pt")
                print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {save_dir / 'model.pt'}")
        if test_dataset:
            eval_candidate_path = os.path.join(os.environ['TEMP_PATH'], 'item_feat_dict_eval.json')
            infer = Infer(args, model, eval_dataset=test_dataset, candidate_path=eval_candidate_path)
            hitrate_eval = infer.infer()
            train_candidate_path = os.path.join(os.environ['TEMP_PATH'], 'item_feat_dict_train.json')
            infer = Infer(args, model, eval_dataset=test_dataset_2, candidate_path=train_candidate_path)
            hitrate_train = infer.infer()

            # è¾“å‡ºç»“æœ
            print("âœ… è¯„ä¼°ç»“æœ")
            print("eval:", hitrate_eval)
            print("train:", hitrate_train)
            # å†™å…¥writer
            writer.add_scalar('HitRat/eval', hitrate_eval, global_step)
            writer.add_scalar('HitRat/train', hitrate_train, global_step)
    return model
def main():
    set_seed(42)

    # ç¯å¢ƒå˜é‡æ£€æŸ¥
    required_env_vars = ['TRAIN_LOG_PATH', 'TRAIN_TF_EVENTS_PATH', 'TRAIN_DATA_PATH', 'USER_CACHE_PATH']
    for var in required_env_vars:
        if not os.environ.get(var):
            raise EnvironmentError(f"ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {var}")

    # åˆ›å»ºæ—¥å¿—ä¸äº‹ä»¶ç›®å½•
    Path(os.environ['TRAIN_LOG_PATH']).mkdir(parents=True, exist_ok=True)
    Path(os.environ['TRAIN_TF_EVENTS_PATH']).mkdir(parents=True, exist_ok=True)

    log_file = open(Path(os.environ['TRAIN_LOG_PATH']) / 'train.log', 'w')
    writer = SummaryWriter(os.path.join(os.environ['TRAIN_TF_EVENTS_PATH'], time.strftime('%Y-%m-%d_%H-%M-%S')))

    args = get_args()
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    args.device = device

    # æ•°æ®åŠ è½½
    data_path = os.environ['TRAIN_DATA_PATH']
    base_dataset = BaseDataset(data_path, args)
    train_idx, valid_idx = base_dataset.split_index([0.9, 0.1])

    # æ„å»ºåµŒå…¥è®­ç»ƒä¸éªŒè¯æ•°æ®é›†
    train_emb_dataset = EmbeddingDataset(base_dataset, train_idx)
    valid_emb_dataset = EmbeddingDataset(base_dataset, valid_idx)
    usernum, itemnum = base_dataset.usernum, base_dataset.itemnum
    feat_stats, feat_types = base_dataset.feat_statistics, base_dataset.feature_types
    final_save_dir = Path(os.environ['USER_CACHE_PATH']) / "embedding_model_final"
    
    # ========== æ¨ç† ==========
    test_dataset = ValidDataset(base_dataset, sample_index=valid_idx)  # å¯æ›¿æ¢ä¸ºç‹¬ç«‹æµ‹è¯•é›†
    test_dataset_2 = ValidDataset(base_dataset, sample_index=train_idx)  # å¯æ›¿æ¢ä¸ºç‹¬ç«‹æµ‹è¯•é›†
    eval_candidate_path = os.path.join(os.environ['TRAIN_DATA_PATH'], 'item_feat_dict.json')
    with open(eval_candidate_path, 'r', encoding='utf-8') as f:
        condidate_data = json.load(f)
    # å¯ä»¥æå‰é¢„çŸ¥å“ªä¸€ä¸ªè¾“å‡º
    eval_candidate_index = []
    for item in test_dataset:
        seq, token_type, seq_feat, user_id, label = item
        eval_candidate_index.append(label[0])

    res_eval_condidate_data = {}
    for item in eval_candidate_index:
        res_eval_condidate_data[str(item)] = condidate_data[str(item)]
    json.dump(res_eval_condidate_data, fp=open(os.path.join(os.environ['TEMP_PATH'], 'item_feat_dict_eval.json'),'w',encoding='utf-8'),indent=4, ensure_ascii=False)

    eval_candidate_index = []
    for item in test_dataset_2:
        seq, token_type, seq_feat, user_id, label = item
        eval_candidate_index.append(label[0])
    eval_candidate_path = os.path.join(os.environ['TEMP_PATH'], 'item_feat_dict_eval.json')

    res_eval_condidate_data = {}
    for item in eval_candidate_index:
        res_eval_condidate_data[str(item)] = condidate_data[str(item)]
    json.dump(res_eval_condidate_data, fp=open(os.path.join(os.environ['TEMP_PATH'], 'item_feat_dict_train.json'),'w',encoding='utf-8'),indent=4, ensure_ascii=False)
    train_candidate_path = os.path.join(os.environ['TEMP_PATH'], 'item_feat_dict_train.json')
    
    if args.use_embedding_model:
        model, final_step = run_embedding_training(
            args=args,
            train_emb_dataset=train_emb_dataset,
            valid_emb_dataset=valid_emb_dataset,
            writer=writer,
            log_file=log_file,
            final_save_dir=final_save_dir,
            device='cuda'
        )
    # åŠ è½½åµŒå…¥æƒé‡åˆ°ä¸‹æ¸¸æ¨¡å‹ï¼ˆç¤ºä¾‹ï¼‰
    downstream_model = DownstreamModel(usernum, itemnum, feat_stats, feat_types, args).to(device)

    if args.use_embedding_model:
        try:
            downstream_model.load_state_dict(torch.load(final_save_dir / "model.pt", map_location=device))
            print("âœ… ä¸‹æ¸¸æ¨¡å‹æˆåŠŸåŠ è½½åµŒå…¥æƒé‡")
        except Exception as e:
            print(f"âŒ ä¸‹æ¸¸æ¨¡å‹åŠ è½½æƒé‡å¤±è´¥: {e}")



    print("æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œç»§ç»­ä¸‹æ¸¸ä»»åŠ¡è®­ç»ƒ...")

    train_dataset = TrainDataset(base_dataset, sample_index=train_idx)  # å…¨é‡è®­ç»ƒ
    valid_dataset = TrainDataset(base_dataset, sample_index=valid_idx)

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        collate_fn=train_dataset.collate_fn
    )
    valid_loader = DataLoader(
        valid_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        collate_fn=valid_dataset.collate_fn
    )

    # æ¨¡å‹åˆå§‹åŒ–
    usernum, itemnum = base_dataset.usernum, base_dataset.itemnum
    feat_stats, feat_types = base_dataset.feat_statistics, base_dataset.feature_types

    model = DownstreamModel(usernum, itemnum, feat_stats, feat_types, args).to(device)

    # åŠ è½½é¢„è®­ç»ƒ embeddingï¼ˆå¯é€‰ï¼‰
    if args.state_dict_path:
        try:
            model.load_state_dict(torch.load(args.state_dict_path, map_location=device))
            print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒæƒé‡: {args.state_dict_path}")
        except Exception as e:
            print(f"âš ï¸ æƒé‡åŠ è½½å¤±è´¥: {e}")

    # ä¼˜åŒ–å™¨ä¸æŸå¤±
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.98))
    bce_criterion = nn.BCEWithLogitsLoss(reduction='mean')

    # å¼€å§‹è®­ç»ƒ
    train_downstream_model(model, train_loader, valid_loader, optimizer, args, log_file, writer,test_dataset, test_dataset_2)

   
    
    eval_candidate_path = os.path.join(os.environ['TEMP_PATH'], 'item_feat_dict_eval.json')
    infer = Infer(args, model, eval_dataset=test_dataset, candidate_path=eval_candidate_path)
    hitrate_eval = infer.infer()
    print("âœ… æ¨ç†å®Œæˆ")
    train_candidate_path = os.path.join(os.environ['TEMP_PATH'], 'item_feat_dict_train.json')
    infer = Infer(args, model, eval_dataset=test_dataset_2, candidate_path=train_candidate_path)
    hitrate_train = infer.infer()

    # è¾“å‡ºç»“æœ
    print("âœ… è¯„ä¼°ç»“æœ")
    print("eval:", hitrate_eval)
    print("train:", hitrate_train)
    # æ¸…ç†èµ„æº
    writer.close()
    log_file.close()
    
if __name__ == '__main__':
    main()